#!/usr/bin/env python
# coding: utf-8

# In[73]:


import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler


# #Chosen Variables:
# - "child_mort" - Death of children under 5 years of age per 1000 live births
# - "income" - Net income per person
# - "gdpp" - The GDP per capita. Calculated as the Total GDP divided by the total population
# 
# Cluster Profiling:
# 
# Cluster profiling is based on major three factors: child mortality, income and GDP.
# 
# Child mortality show access to basic necessities that lower child mortality, such as nutrition, basic hygiene and basic healthcare. Child mortality is related to total fertility (Factfulness, Hans Rosling). Child mortality is said to be the best predictor of quality of life. (Numbers Don't Lie - 71 Things You Need to Know About the World, Vaclav Smil). Other variables have been chosen based on the graphical analysis.
# 

# # Exploratory Analysis:

# In[74]:


df = pd.read_csv("Country-data.csv")


# Creating a new dataframe for these variables. 

# In[75]:


countries = df[["country", "child_mort", "income", "gdpp"]]

countries_K = df[["child_mort", "income", "gdpp"]] #numeric data set needed for KMeans segmentation


# In[76]:


countries.dtypes


# In[77]:


countries.isnull().sum()


# In[78]:


countries.shape


# # Graphical Analysis:

# ### Analysis in order to find 3 variables which will be used in KMeans segmentation method

# In[79]:


sns.pairplot(df, diag_kind="kde", kind='reg', 
             plot_kws={'line_kws':{'color':'red'}})
plt.show()


# ## We need to transform the data in order to assess linear relationship between variables

# In[80]:


df["inflation"]=abs(df["inflation"])
df["inflation"]
df.drop(columns="country", inplace=True)


# In[81]:


#Unskewing data with Box-Cox transformation

def boxcox_df(x):    
    x_boxcox, _ = stats.boxcox(x)
    return x_boxcox
df_boxcox = df.apply(boxcox_df, axis=0)


# In[82]:


sns.pairplot(df_boxcox, diag_kind="kde", kind='reg', 
             plot_kws={'line_kws':{'color':'red'}})
plt.show()


# 
# On the chart above we can observe the following relations between variables: 
# 
# 
# 1.   Strong Linear relation can be found between:
# 
# *   **imports** - **exports**
# *   **total_fer** - **child_mort**
# *   **gdpp** - **income**
# 
# 
# 2.   Non-linear relation namely rectangular hyperbola curve between:
# *   **gdpp** - **child_mort**
# 3. We can also notice that the bigger GDP relates to the:
# * lower  child mortality 
# * higger income 
# * lower inflation
# * longer life expectancy
# * lower total fertility
# 

# In[83]:


plt.figure(figsize = (25,15))
ax = sns.heatmap(df_boxcox.corr(),square = True,annot=True, cmap="Greens")
bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5);


# Above we can see a heatmap with correlations between variables.
# The most important  insights from it are as follows: 
# 
# *   **child_mort** is having high negative correlation with **life_expec**
# *   **health**, **exports**, **income**,**imports** are highly correlated with **gdpp**
# * **exports** is highly correlated with **imports**
# * **total_fer** is highly positively correlated with **child_mort** and negatively correlated with **life_expec**

# 
# ## Relationship between quantitative variables ('child_mort','income', 'gdpp')

# In[84]:


sns.pairplot(countries, diag_kind="kde", kind='reg', 
             plot_kws={'line_kws':{'color':'red'}})
plt.show()


# Rectangular hyperbola curve is generated by gdpp-child_mort. Estimated distributions on the diagonal are **highly skewed** which means they are **not normally distributed**
# 

# ## Unskewing data with Box-Cox transformation (quantitative variables ('child_mort','income', 'gdpp'))

# In[85]:


#Unskewing data with Box-Cox transformation

def boxcox_df(x):    
    x_boxcox, _ = stats.boxcox(x)
    return x_boxcox
countries_boxcox = countries_K.apply(boxcox_df, axis=0)


# In[86]:


sns.pairplot(countries_boxcox, diag_kind='kde', kind='reg', 
             plot_kws={'line_kws':{'color':'red'}})
plt.show()


# By using Box-Cox transformation we have successfully unskewed the variables and they now are almost normally distributed.

# ## Scaling the data

# Scaling works by first subtracting the column average from each individual entry. 
# This step ensures the column average is adjusted to zero. Then we divide the result by each column's standard deviation. This step makes the column's standard deviation equal to 1.
# We  transform the unskewed dataset  to the same scale, meaning all columns have a mean of zero, and standard deviation of 1

# In[87]:


#Scaling the data
scaler = StandardScaler()
scaler.fit(countries_boxcox)
countries_scaled = scaler.transform(countries_boxcox)
countries_scaled_df = pd.DataFrame(data=countries_scaled,index=countries_boxcox.index,columns=countries_boxcox.columns)


# In[88]:


countries_scaled_df.agg(['mean','std']).round()


# # Elbow criterion method to decide how many clusters to use

# K-means require  value to be set beforehand. 
# There are two ways to define k - either mathematically or by testing different values and exploring the results.
# 
#  We will first test the mathematical approach by using the elbow criterion method. This will get us a ball-park estimate of what is the optimal number of clusters.
# 
#  
# We plot the sum of squared errors again k to identify the so-called elbow - where the decrease in the errors slows down, and there's only incremental reduction with more segments past that point.
# 
# First we initialize an empty dictionary called sse.
#  Then, we run a loop through k values between 1 and 11, and store the sum of squared errors that can be accessed with inertia underscore argument from the fitted kmeans model. Finally, we plot the k values or keys argument of the sse dictionary on the x axis, and the sum of squared errors or values argument in the sse dictionary on the y axi
# 
# 
# 

# In[89]:


sse = {}

for k in range(1, 11):
    kmeans=KMeans(n_clusters=k, random_state=333)
    kmeans.fit(countries_scaled_df)
    sse[k] = kmeans.inertia_

plt.title('Elbow criterion method chart')
sns.pointplot(x=list(sse.keys()), y=list(sse.values()))
plt.show()



# 
# You can see that the "elbow" is somewhere around 2 or 3 clusters, which means it should start cluster with +1 number of clusters i.e. with 3 or 4. We always chose lower so k=3
# 
# 

# # Building segmentation using KMeans

# In[90]:


# Initialize `KMeans` with 3 clusters
kmeans=KMeans(n_clusters=3, random_state=123)

# Fit the model on the pre-processed dataset
kmeans.fit(countries_scaled_df)

# Assign the generated labels to a new column
countries_kmeans3 = countries.assign(segment = kmeans.labels_)


# In[91]:


#below [countries_clusters] is needed in order to retrieve the country connected with the segment
cluster_K = pd.DataFrame(kmeans.labels_, columns = ['segment'])


# In[92]:


countries_clusters = pd.concat([countries, cluster_K], axis=1)
countries_clusters.head()


# In[93]:


#Analyzing average K-means segmentation attributes
kmeans3_averages = countries_kmeans3.groupby(['segment']).mean().round(0)
print(kmeans3_averages)


# # Cluster visualization
# On the charts below we can observe the differences between 
# '**gdpp**', '**child_mort**' and '**income**' in presented clusters of countries. Analisys of those clusters can allow us to distinguish  clusters of developed countries from the clusters of under-developed countries.
# 

# In[94]:


# Scatter-Plot : 

f, axes = plt.subplots(1, 3, figsize=(20,5))
sns.scatterplot(x='income', y='child_mort', hue='segment', data=countries_kmeans3, palette='Set1',ax=axes[0]);
sns.scatterplot(x='gdpp', y='income', hue='segment', data=countries_kmeans3, palette='Set1',ax=axes[1]);
sns.scatterplot(x='gdpp', y='child_mort', hue='segment', data=countries_kmeans3, palette='Set1',ax=axes[2]);


# Homogeneity analisys of segments presented on charts above: 
# 
# We can observe that: 
# * Countries clustered in "Semgent 0" are homogeneous taking into consideration **gdpp** & **income**. However the **child_mort** rates an differ quite a lot. 
# * Segment 2 is the most homogeneous one.
# * Countries clustered in "Semgent 1" are homogeneous taking into consideration **child_mort**. 
# However the **gdpp** & **income** values can differ inside the segment significantly. 
# 
# 

# #### Visualising the profiled variables of the dataset via barplot

# In[95]:


kmeans3_averages.plot(kind='bar', logy=True)


# What we can learn from those charts? 
# 
# 
# * Countries with high gdpp, high income and low child mortality we can label as **Developed countries** 
# * Countries with low gdpp,income and low child mortality are D**eveloping countries**
# * Countries with low gdpp,low income and high child mortality cab be labeled as **Under-developed countries**
# 
# 
# 
